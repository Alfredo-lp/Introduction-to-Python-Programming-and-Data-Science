{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "def css_styling():\n",
    "    styles = open(\"../Data/www/styles/custom.css\", \"r\").read()\n",
    "    return HTML(styles)\n",
    "css_styling()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interacting with the Web (Part I)\n",
    "\n",
    "The Internet is a gigantic data dump. There is all the social networking data from Facebook, Twitter, and so on. There is the news from all the traditional media sources plus Quartz, Vox, and so on. Then there is the data from organizations such as the World Bank, the Bureau of Labor Statistics, the US Census, or Chicago's Data Portal.  Finally, you have all your scientific data sources: the National Cancer Institute, the ProteinBank, or the Kyoto Gene and Genomes Encyclopedia.\n",
    "\n",
    "How can you use Python to access those sites and retrieve data for your research, your business, or your hobby?\n",
    "\n",
    "There are two main approaches to retrieve data from websources. The preferred approach is using **Application Program Interfaces** or APIs.  If an organization has decided to share its data, and they have the forethought and resources to do it, they will develop an API that will let you interact with their data.\n",
    "\n",
    "If the organization does not have the forethought or resources to create an API (or if they do not want to share their data), then you have to **crawl** their website and **scrape** their data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+++"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application Program Interfaces\n",
    "\n",
    "** We relied heavily for these materials on https://www.dataquest.io/blog/python-api-tutorial/**\n",
    "\n",
    "APIs simplify the process of obtaining specific information from a data source.  You do not have to worry about figuring out the **format** in which the information is stored, or **where** the information is stored.  All of those matter are handled seamlessly by the API. \n",
    "\n",
    "But convenience is not the only advantage of an API. APIs are also particular useful when:\n",
    "\n",
    "* You want a small piece of a much larger set of data. Reddit comments are one example. What if you want to just pull your own comments on Reddit? It doesn’t make much sense to download the entire Reddit database, then filter just your own comments.\n",
    "    \n",
    "* There is repeated computation involved. Spotify has an API that can tell you the genre of a piece of music. You could theoretically create your own classifier, and use it to categorize music, but you’ll never have as much data as Spotify does.\n",
    "    \n",
    "* The data is changing quickly. An example of this is stock price data. It doesn’t really make sense to regenerate a dataset and download it every minute – this will take a lot of bandwidth, and be pretty slow.\n",
    "    \n",
    "    \n",
    "    \n",
    "### Making a request\n",
    "\n",
    "In order to learn how APIs work, we will first use the APIs developed to retrieve data on the **International Space Station (ISS)**.  The relevant APIs can be found at http://open-notify.org/.  We will first consider the API for retrieving the location (latitude and longitude) of the ISS (http://open-notify.org/Open-Notify-API/ISS-Location-Now/). The API is hosted at http://api.open-notify.org/iss-now.json. \n",
    "\n",
    "So, how do we make requests for information with this API?\n",
    "\n",
    "Like standard webpages, APIs are also hosted on web servers. When you type http://www.google.com in your browser’s address bar, your computer is actually asking the http://www.google.com server for a webpage, which it then returns it to your browser for display. That action is called a `request`. APIs work much the same way, except instead of your web browser asking for a webpage, your program asks for **data**. This data is usually returned in JSON format.\n",
    "\n",
    "There are many possible types of requests. The most common, and the one we will be using throughout this unit, is the `GET` request. A `GET` request simply accesses and downloads the webpage found at the URL you specified as an input. \n",
    "\n",
    "We will use the package [`requests`](http://docs.python-requests.org/en/latest/user/quickstart/) package to crawl (load) webpages and scrape (download) their contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(\"http://api.open-notify.org/iss-now.json\")\n",
    "print( response )\n",
    "print( response.status_code )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Methods from the `requests` package return `Response` objects. One of the most important properties of the response is its `status code`, which is printed by default but which we can also get explicitly.\n",
    "\n",
    "Here are some of the most common status codes you might encounter:\n",
    "* 200, **OK**. Standard response for successful HTTP requests. The actual response will depend on the request method used.\n",
    "* 301, **Moved Permanently**. The server is redirecting you to a different endpoint. This and all future requests should be directed to the given URL. This can happen when a company switches domain names, or an endpoint name is changed.\n",
    "* 303, **See Other**. The response to the request can be found under another URI using a GET method. When received in response to a POST (or PUT/DELETE), the client should presume that the server has received the data and should issue a redirect with a separate GET message. Your web browser automatically fetches the new URL but web crawlers do not usually do this unless you specify it.\n",
    "* 400, **Bad Request**. The server cannot or will not process the request due to an apparent client error (e.g., malformed request syntax, invalid request message framing, or deceptive request routing).\n",
    "* 401, **Unauthorized**. Similar to `403 Forbidden`, but specifically for use when authentication is required and has failed or has not yet been provided. The response must include a WWW-Authenticate header field containing a challenge applicable to the requested resource.\n",
    "* 403, **Forbidden**. The request was a valid request, but the server is refusing to respond to it. `403` error semantically means \"unauthorized\", i.e. the user does not have the necessary permissions for the resource.\n",
    "* 404, **Not Found**. The requested resource could not be found but may be available in the future. Subsequent requests by the client are permissible.\n",
    "* 500, **Internal Server Error**. A generic error message, given when an `unexpected` condition was encountered and no more specific message is suitable.\n",
    "* 503, **Service Unavailable**. The server is currently unavailable (because it is overloaded or down for maintenance). Generally, this is a temporary state.\n",
    "* 504, **Gateway Timeout**. The server was acting as a gateway or proxy and did not receive a timely response from the upstream server.[\n",
    "\n",
    "\n",
    "\n",
    "More codes: http://en.wikipedia.org/wiki/List_of_HTTP_status_codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The status code of our request was **200**. It means that all went well -- we successfully connected to the web address we wanted and downloaded its contents.\n",
    "\n",
    "But `status codes` are not the only methods available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( response.url )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( response.text )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the content format specified http://open-notify.org/Open-Notify-API/ISS-Location-Now/. It is in `json` format which means that we can easily parse it using the `json` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = json.loads(response.text)\n",
    "\n",
    "print( data )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**YES**. \n",
    "\n",
    "The method `loads()` returns json formatted data as a dictionary. We can print whatever information we need from the dictionary using the appropriate `keys`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['iss_position']['latitude']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( \"The ISS current position is {0:6.3f} of latitude and {1:6.3f} of longitude.\".format( \n",
    "        float(data['iss_position']['latitude']), \n",
    "        float(data['iss_position']['longitude']) ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another example\n",
    "\n",
    "The [`Open Notify`](http://open-notify.org/) API can handle different kinds of data requests. Let's now consider the case overhead pass prediction API (http://open-notify.org/Open-Notify-API/ISS-Pass-Times/).  Before we write any code, it is important to check the requirements of the API. \n",
    "\n",
    "**Overview**\n",
    "\n",
    "`The API returns a list of upcoming ISS passes for a particular location formatted as JSON.`\n",
    "\n",
    "`As input it expects a latitude/longitude pair, altitude and how many results to return. All fields are required.`\n",
    "\n",
    "`As output you get the same inputs back (for checking) and a time stamp when the API ran in addition to a success or failure message and a list of passes. Each pass has a duration in seconds and a rise time as a unix time stamp.`\n",
    "\n",
    "Notice the second line. We need to provide inputs. Let's see how we can do this!\n",
    "\n",
    "Any suggestions?\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, we need to read the documentation for [requests](http://docs.python-requests.org/en/master/user/quickstart/#passing-parameters-in-urls). Specifically, the documentation concerning passing parameters.\n",
    "\n",
    "This is the relevant information: `Requests allows you to provide these arguments as a dictionary, using the params keyword argument.`\n",
    "\n",
    "And what are the parameters we need to provide? The API's documentation has the answer:\n",
    "\n",
    "**Input**\n",
    "\n",
    "`This API has 2 required input values and 2 optional ones.`\n",
    "\n",
    "`Inptut\tDescription\tQuery string\tValid Range\tUnits\tRequired?`\n",
    "\n",
    "`Latitude\tThe latitude of the place to predict passes\tlat\t-80..80\tdegrees\tYES`\n",
    "\n",
    "`Longitude\tThe longitude of the place to predict passes\tlon\t-180..180\tdegrees\tYES`\n",
    "\n",
    "`Altitude\tThe altitude of the place to predict passes\talt\t0..10,000\tmeters\tNo`\n",
    "\n",
    "`Number\tThe number of passes to return\tn\t1..100\t–\tNo`\n",
    "\n",
    "The beauty of code documentation. If you recall, the `Overview` stated that all inputs were required. The `Inputs` section let's know that only `lat` and `lon` are actually required.  Let's try and see what happens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#our_location = {'lat': , 'lon': , 'alt': , 'n:' }\n",
    "\n",
    "our_location = {'lat': 41.878, 'lon': -87.6298}\n",
    "\n",
    "response = requests.get(\"http://api.open-notify.org/iss-pass.json\", params = our_location)\n",
    "\n",
    "print( response.url )\n",
    "print( response.content )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What have we learned?** \n",
    "\n",
    "That indeed providing values for `alt` and `n` is not required.  The APIs just assigns those variables default values of `100` and `5`, respectively.\n",
    "\n",
    "We can also see that, as stated in the documentation, the output returns both the values of our inputs and the data we are requesting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** It is now time for you to try to use an API on your own. The last API available at [`Open Notify`](http://open-notify.org/) returns the number of astronauts in the ISS. Write the code to access that information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    "## Using The US Census' API\n",
    "\n",
    "The United States Census is a decennial census mandated by the United States Constitution. The United States Census Bureau (officially the Bureau of the Census) is responsible for the United States Census.\n",
    "\n",
    "The first census after the American Revolution was taken in 1790, under Secretary of State Thomas Jefferson; there have been 22 federal censuses since that time. The current national census was held in 2010; the next census is scheduled for 2020 and will be largely conducted using the Internet. For years between the decennial censuses, the Census Bureau issues estimates made using surveys and statistical models.\n",
    "\n",
    "The Census Bureau has begun rolling out their datasets via [APIs](http://www.census.gov/developers/). You can find a full list of APIs [here](http://www.census.gov/data/developers/data-sets.html).  In this unit, we will focus on the [decennial census](http://www.census.gov/data/developers/data-sets/decennial-census-data.html).\n",
    "\n",
    "Because we are dealing with US data, we will start by loading some helpful data: US city names, their states, and their geographic codes.  The relevant data is stored in `json` format in `../Data/Day6-Web-Scraping/`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../Data/Day6-Web-Scraping/us_state_names.json') as file_in:\n",
    "    states_w_codes = json.load( file_in )\n",
    "    \n",
    "with open('../Data/Day6-Web-Scraping/us_state_city_names.json') as file_in:\n",
    "    cities_by_state = json.load( file_in )\n",
    "\n",
    "print(states_w_codes.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(states_w_codes['PA'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FIPS state codes** are numeric and two-letter alphabetic codes defined in U.S. Federal Information Processing Standard Publication (\"FIPS PUB\") 5-2 to identify U.S. states and certain other associated areas. The codes are used in Geographic Names Information System, overseen by the U.S. Board on Geographic Names. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cities_by_state.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(states_w_codes['MT'])\n",
    "print(cities_by_state['MT']['Antelope'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the basic information, we can start using the API to retrieve data. The Census Bureau has a number of helpful resources.  The [decennial census page](http://www.census.gov/data/developers/data-sets/decennial-census-data.html) constains basic instructions on how to contruct queries. There is a also a [page with an example](https://api.census.gov/data/2010/dec/sf1?get=PCT012A015,PCT012A119&for=state:01), and a page with a list of all (and I *really* mean **all**) [variable codes](https://api.census.gov/data/2010/dec/sf1/variables.html).\n",
    "\n",
    "But before we can do anything, we need to obtain a `key` that will identify us as the person doing the queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('census_auth.json', 'r') as file_in:\n",
    "    auth = json.load(file_in)\n",
    "    \n",
    "#print(auth['my_key'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "census_url = 'https://api.census.gov/data/2010/dec/sf1'\n",
    "\n",
    "\n",
    "# Example from Census: http://api.census.gov/data/2010/sf1?key=b48301d897146e8f8efd9bef3c6eb1fcb864cf&get=P0010001,NAME&for=state:*\n",
    "\n",
    "response = requests.get( census_url, params = {'key': auth['my_key'], 'get': 'P001001,NAME', 'for': 'state: *'})\n",
    "print(response.status_code)\n",
    "print('http://api.census.gov/data/2010/sf1?key=1eb4e956ec8bdfd987960641728d0fed68589575&get=P0010001,NAME&for=state:*')\n",
    "print(response.url)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also write queries that obtain several data sets all at once. For example, we can obtain population by age and ethnicity using the codes:\n",
    "\n",
    "* P012A018 -- Sex By Age (White Alone) MALE 60 yrs old\n",
    "* P012A038 -- Sex By Age (White Alone) FEMALE 40-44 yrs old\n",
    "* P012B018 -- Sex By Age (Black Or African American Alone) MALE 60 yrs old\n",
    "\n",
    "And we can restrict the query to a single state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_codes = ''\n",
    "for i in ['P012A018', 'P012A038', 'P012B018']:\n",
    "    data_codes += i + ','\n",
    "data_codes += 'NAME'\n",
    "print(data_codes)\n",
    "\n",
    "state_fips = 'state:' + states_w_codes['AK']['fips_state']\n",
    "\n",
    "response = requests.get( census_url, params = {'key': auth['my_key'], 'get': data_codes, 'for': state_fips})\n",
    "print(response.status_code)\n",
    "print(response.url)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also retrieve the population for specific cities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_codes = 'place:'\n",
    "for city in ['Chicago', 'Evanston', 'Wilmette']:\n",
    "    location_codes += '0'+ cities_by_state['IL'][city]['fips_city'] + ','\n",
    "location_codes = location_codes[:-1]\n",
    "\n",
    "state_fips = 'state:' + states_w_codes['IL']['fips_state']\n",
    "\n",
    "response = requests.get( census_url, params = {'key': auth['my_key'], 'get': data_codes, 'for': location_codes, 'in': state_fips})\n",
    "\n",
    "print(response.status_code)\n",
    "print(response.url)\n",
    "print(response.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refactor our code \n",
    "\n",
    "We have written code that can retrieve specific decennial census information, however, that code is not modular or generalizable. In order to write better code it is useful to refactor our code so it is modular and generalizable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_query_for_census_API(ages, cities, state_code, census_key, ethnicity_code = 'A' ):\n",
    "    \"\"\"\n",
    "    Creates a query for retrieving male populations of given ethnicity for a given set of cities\n",
    "    \n",
    "    input:\n",
    "        ages - list : ages of male population to query\n",
    "        cities - list : fips codes of cities to query \n",
    "        state_code - str : fips code of state for cities\n",
    "        census_key - str : user personal key for census API\n",
    "        ethnicity_code - str : ethnicity census code (A, B, C, D, H)\n",
    "        \n",
    "    output:\n",
    "        query - dict : params for API query\n",
    "    \"\"\"\n",
    "    \n",
    "    return query\n",
    "\n",
    "reponse = requests.get( census_url, params = create_query_for_census_API() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
